from sentence_transformers import SentenceTransformer
from chromadb import PersistentClient
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage
from dotenv import load_dotenv
import os

# .env dosyasÄ±nÄ± yÃ¼kle
load_dotenv()

# OpenRouter ayarlarÄ±
os.environ["OPENAI_API_BASE"] = "https://openrouter.ai/api/v1"
os.environ["OPENAI_API_KEY"] = os.getenv("OPENROUTER_API_KEY")

# Embedding modeli
embedding_model = SentenceTransformer("intfloat/multilingual-e5-base")

# ChromaDB baÄŸlantÄ±sÄ±
chroma_client = PersistentClient(path="./chroma_haber")
collection = chroma_client.get_or_create_collection("haber_vektor")

# LLM modeli (Google Gemma)
llm = ChatOpenAI(
    model="google/gemma-3-27b-it:free",
    temperature=0.4,
    max_tokens=300
)

# KullanÄ±cÄ±dan soru al
soru = input("â“ Sorunuzu yazÄ±n: ")

# Soru vektÃ¶rleÅŸtirme
soru_vektoru = embedding_model.encode(soru).tolist()

# En alakalÄ± 3 haberi getir
sonuclar = collection.query(
    query_embeddings=[soru_vektoru],
    n_results=3
)

# Belgeleri birleÅŸtir
belgeler = sonuclar["documents"][0]
icerik = "\n\n".join(belgeler)

# Prompt oluÅŸtur
prompt = f"""AÅŸaÄŸÄ±daki haber iÃ§eriklerine gÃ¶re soruya cevap ver fakat cevaplarÄ± biraz daha ayrÄ±ntÄ±lÄ± verebilirsin. Daha fazla detaylandÄ±r cevaplarÄ±:

Haber Ã–zeti Ä°Ã§eriÄŸi:
{icerik}

Soru:
{soru}

Cevap:"""

# LLM'den cevap al
yanit = llm([HumanMessage(content=prompt)])

# CevabÄ± yazdÄ±r
print("\nğŸ§  LLM YanÄ±tÄ±:\n", yanit.content.strip())
